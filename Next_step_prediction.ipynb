{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elainedias16/android-layout/blob/master/Next_step_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ku2I-_TlrUyM",
        "outputId": "ffd6f686-da0e-42a6-efd2-ac8095c82127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architeture\n",
        "https://dugas.ch/artificial_curiosity/GPT_architecture.html\n",
        "\n",
        "https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter7/6\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/0\n",
        "\n",
        "https://debuggercafe.com/text-generation-with-transformers/"
      ],
      "metadata": {
        "id": "wHlD7QV0c7RL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoY2CAyQPeWF"
      },
      "source": [
        "## Masked Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale dot produt attetion:\n",
        "https://paperswithcode.com/method/scaled"
      ],
      "metadata": {
        "id": "YyczlNf4nndV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.key = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "    self.value = nn.Linear(config.d_model, config.head_dim, bias=config.bias)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    return q, k, v\n",
        "\n",
        "\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.d_model = config.d_model\n",
        "        self.heads = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n",
        "        self.output_linear = nn.Linear(config.d_model, config.d_model)\n",
        "        assert self.head_dim * self.num_heads == self.d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # B, T, C = x.size()\n",
        "\n",
        "        heads_output = []\n",
        "        for head in self.heads:\n",
        "            k, q, v = head(x)\n",
        "\n",
        "\n",
        "            # Scaled dot-product attention\n",
        "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "            if mask is not None:\n",
        "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "            attn_weights = F.softmax(scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "            head_output = torch.matmul(attn_weights, v)\n",
        "            heads_output.append(head_output)\n",
        "\n",
        "\n",
        "        concatenated_output = torch.cat(heads_output, dim=-1)\n",
        "        output = self.output_linear(concatenated_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NbvfI4tFlQ_u"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNfea0_8oLZw"
      },
      "source": [
        "## Feed Forward Nerual Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QyoN_gBEzPM"
      },
      "source": [
        "\n",
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "dVxd-AS5ok8v"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(config.d_model, 4 * config.d_model, bias=config.bias)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(config.d_model * 4,  config.d_model, bias=config.bias)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.linear2(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8FHuZBKLfMP"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "hjA6NoboLhpe"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(config.d_model, config.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.norm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t72dRbgtM6Xy"
      },
      "source": [
        "## One Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "4RAoljJTM8Al"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = LayerNorm(config)\n",
        "    self.masked_self_attention = MaskedSelfAttention(config)\n",
        "    self.ln_2 = LayerNorm(config)\n",
        "    self.feed_forward = FeedFoward(config)\n",
        "\n",
        "  # def forward(self, x, mask):\n",
        "  #   x = self.ln_1(x)\n",
        "  #   x = x + self.masked_self_attention(x, mask)\n",
        "  #   x = self.ln_2(x)\n",
        "  #   x = x + self.feed_forward(x)\n",
        "  #   return x\n",
        "  def forward(self, x):\n",
        "    x = self.ln_1(x)\n",
        "    x = x + self.masked_self_attention(x)\n",
        "    x = self.ln_2(x)\n",
        "    x = x + self.feed_forward(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uluREArFtEP-"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@hunter-j-phillips/positional-encoding-7a93db4109e6#:~:text=class%20PositionalEncoding(nn,self.dropout(x)\n",
        "olhar posiitional"
      ],
      "metadata": {
        "id": "S9mAVQgzvNCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(config.max_length, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.blocks = nn.Sequential(*[Decoder(config) for _ in range(config.n_layer)])\n",
        "        self.ln = LayerNorm(config)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        device = input_ids.device\n",
        "        B, T = input_ids.size()\n",
        "\n",
        "        # Positional e token embed\n",
        "        tok_emb = self.word_token_embedding(input_ids)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "\n",
        "        # Transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Norm layer\n",
        "        x = self.ln(x)\n",
        "\n",
        "        # Final layer\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Shift logits and targets to align\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_targets = targets[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "    def generate(self, input_ids, max_new_tokens):\n",
        "        new_tokens = []\n",
        "\n",
        "        for _ in range(0, max_new_tokens):\n",
        "            input_ids_cond = input_ids[:, -self.config.block_size:]\n",
        "            logits, _ = self.forward(input_ids_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            input_ids_next = torch.multinomial(probs, num_samples=1)\n",
        "            new_tokens.append(input_ids_next)\n",
        "            input_ids = torch.cat((input_ids, input_ids_next), dim=1)\n",
        "\n",
        "        new_tokens = torch.cat(new_tokens, dim=1)\n",
        "        return new_tokens\n"
      ],
      "metadata": {
        "id": "ftxqAuuVC_V7"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XzNJw5itApm"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "collapsed": true,
        "id": "XVgYPtEGCfoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#Paramters:\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "max_new_tokens = 50\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 8\n",
        "SEQUENCE_LENGTH = 64\n",
        "\n",
        "\n",
        "class Config:\n",
        "    num_heads = 2\n",
        "    d_model = 8 #os vetores de entrada e saída terão dimensão 8\n",
        "    head_dim = 4 #cada cabeça tem dimensão 4\n",
        "    dropout = 0.1  #para evitar overfiting\n",
        "    bias = True\n",
        "    vocab_size = 50257  # len tokenizer\n",
        "    # hidden_size = 1024\n",
        "    max_length = 512\n",
        "    n_layer = 6\n",
        "    # block_size = 1024\n",
        "    # block_size = 32\n",
        "    block_size = SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "TMRf3-h4hv0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-o00IRzGHdt",
        "outputId": "9450f878-d42b-46b2-fc6e-e9070c14b3dc"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation"
      ],
      "metadata": {
        "id": "bXIZ2i_pqbcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cjNv2vmzqhSw",
        "outputId": "56dd7651-a2a3-41fb-8792-c00f46327f3c"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-02 01:20:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.8’\n",
            "\n",
            "\rinput.txt.8           0%[                    ]       0  --.-KB/s               \rinput.txt.8         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-02 01:20:34 (8.53 MB/s) - ‘input.txt.8’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "9_XyrK4xF2h3"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  return word_tokenize(text.lower())\n",
        "\n",
        "\n",
        "def build_vocab(tokens):\n",
        "  word_counts = Counter(tokens)\n",
        "  vocab = list(word_counts.keys())\n",
        "  word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "  int_to_word = {i: word for word, i in word_to_int.items()}\n",
        "  return len(vocab), word_to_int, int_to_word\n",
        "\n",
        "\n",
        "\n",
        "def prepare_data(text, word_to_int, sequence_length=64):\n",
        "  # Tokenize the text\n",
        "  tokens = tokenize(text)\n",
        "  # Encode the tokens into integers\n",
        "  encoded = [word_to_int[token] for token in tokens]\n",
        "  # Create training samples\n",
        "  samples = [encoded[i:i + sequence_length + 1] for i in range(len(encoded) - sequence_length)]\n",
        "  # Separate input and target sequences\n",
        "  input_ids = torch.tensor([sample[:-1] for sample in samples], dtype=torch.long)\n",
        "  targets = torch.tensor([sample[1:] for sample in samples], dtype=torch.long)\n",
        "  return input_ids, targets\n",
        "\n",
        "\n",
        "def tokenize_and_encode(text, word_to_int):\n",
        "    tokens = tokenize(text)\n",
        "    # Add a check if '<pad>' is in the vocabulary\n",
        "    if '<pad>' not in word_to_int:\n",
        "        # If not, assign it a default value (e.g., 0)\n",
        "        pad_id = 0\n",
        "    else:\n",
        "        pad_id = word_to_int['<pad>']\n",
        "    encoded = [word_to_int.get(token, pad_id) for token in tokens]\n",
        "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "\n",
        "def decode_tokens(tokens, int_to_word):\n",
        "    return ' '.join([int_to_word[token.item()] for token in tokens.squeeze()])\n",
        "\n",
        "\n",
        "def generate_text(model, start_text, word_to_int, int_to_word, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenize_and_encode(start_text, word_to_int)\n",
        "        generated_tokens = model.generate(input_ids, max_new_tokens)\n",
        "        return decode_tokens(generated_tokens, int_to_word)\n",
        "\n",
        "\n",
        "tokenize_text = tokenize(text)\n",
        "vocab_size, word_to_int, int_to_word = build_vocab(tokenize_text)"
      ],
      "metadata": {
        "id": "tep1DGVOsuz6"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "o6U41h0iFgiT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3SwMGJDu_i3"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "d_gmhQ1wFoRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(config)\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "eFFbhwnYGzCF"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "skabJPjcNfXe"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('alice_1.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# text = \"Alice was a curious and imaginative young girl\"\n",
        "\n",
        "def train_model(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(input_ids, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "\n",
        "tokens = tokenize(text)\n",
        "config.vocab_size, word_to_int, int_to_word = build_vocab(tokens)\n",
        "print(f\"vocab_size: {config.vocab_size}\")\n",
        "input_ids, targets = prepare_data(text, word_to_int)\n",
        "print(f\"input_ids shape: {input_ids.shape}\")\n",
        "print(f\"targets shape: {targets.shape}\")\n",
        "\n",
        "dataset = TextDataset(input_ids, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Transformer(config)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_model(model, dataloader, optimizer, epochs=epochs)\n",
        "\n",
        "start_text = \"Alice was walking through the forest when she suddenly saw a strange\"\n",
        "generated_text = generate_text(model, start_text, word_to_int, int_to_word, max_new_tokens=max_new_tokens)\n",
        "print(generated_text)\n",
        "\n",
        "\n",
        "####################Test##################################\n",
        "# test_sentences = [\n",
        "#     \"First Citizen: Before we proceed any further, hear me speak.\",\n",
        "#     \"All: Speak, speak.\",\n",
        "#     \"First Citizen: You are all resolved rather to die than to famish?\",\n",
        "#     \"All: Resolved. resolved.\",\n",
        "#     \"First Citizen: First, you know Caius Marcius is chief enemy to the people.\",\n",
        "#     \"All: We know't, we know't.\"\n",
        "# ]\n",
        "\n",
        "test_sentences = [\n",
        "    \"Alice was walking through the forest when she suddenly saw a strange.\",\n",
        "    \"As Alice pleaded with the Queen for help\",\n",
        "    \"Continuing her exploration, Alice encountered \",\n",
        "    \"Alice delved into books\",\n",
        "    \"First Citizen: First, you know Caius Marcius is chief enemy to the people.\",\n",
        "    \"Alice began to explore lucid dreaming\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFev0y6JFniD",
        "outputId": "c90c5f17-f8f2-4528-8f6e-68fb249e8314"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 375\n",
            "input_ids shape: torch.Size([789, 64])\n",
            "targets shape: torch.Size([789, 64])\n",
            "Epoch 1, Loss: 5.847218918077873\n",
            "Epoch 2, Loss: 5.112595062063198\n",
            "Epoch 3, Loss: 4.918454526650785\n",
            "Epoch 4, Loss: 4.79877830755831\n",
            "Epoch 5, Loss: 4.597837062797161\n",
            "Epoch 6, Loss: 4.328755633999603\n",
            "Epoch 7, Loss: 4.055418250536678\n",
            "Epoch 8, Loss: 3.813100212752217\n",
            "Epoch 9, Loss: 3.5862612001823657\n",
            "Epoch 10, Loss: 3.3894923287208636\n",
            "Epoch 11, Loss: 3.206034188318734\n",
            "Epoch 12, Loss: 3.045413299040361\n",
            "Epoch 13, Loss: 2.897535391528197\n",
            "Epoch 14, Loss: 2.7789604952841094\n",
            "Epoch 15, Loss: 2.665606920165245\n",
            "Epoch 16, Loss: 2.5610175084586095\n",
            "Epoch 17, Loss: 2.4807915037328545\n",
            "Epoch 18, Loss: 2.3927885932151716\n",
            "Epoch 19, Loss: 2.329069758906509\n",
            "Epoch 20, Loss: 2.2666763898098106\n",
            "Epoch 21, Loss: 2.2119817613351223\n",
            "Epoch 22, Loss: 2.168883954635774\n",
            "Epoch 23, Loss: 2.1115310047612046\n",
            "Epoch 24, Loss: 2.061911855081115\n",
            "Epoch 25, Loss: 2.0308367579874367\n",
            "Epoch 26, Loss: 1.9896339178085327\n",
            "Epoch 27, Loss: 1.9537871016396418\n",
            "Epoch 28, Loss: 1.908970240390662\n",
            "Epoch 29, Loss: 1.8748216641069664\n",
            "Epoch 30, Loss: 1.853488240579162\n",
            "Epoch 31, Loss: 1.8141019862107557\n",
            "Epoch 32, Loss: 1.7949591273009176\n",
            "Epoch 33, Loss: 1.767653712118515\n",
            "Epoch 34, Loss: 1.7504907027639525\n",
            "Epoch 35, Loss: 1.724344455834591\n",
            "Epoch 36, Loss: 1.6807123386498652\n",
            "Epoch 37, Loss: 1.6654262434352527\n",
            "Epoch 38, Loss: 1.6531709565056696\n",
            "Epoch 39, Loss: 1.6394969068392358\n",
            "Epoch 40, Loss: 1.6172586053308815\n",
            "Epoch 41, Loss: 1.6010067258218321\n",
            "Epoch 42, Loss: 1.584189357179584\n",
            "Epoch 43, Loss: 1.5679906690963592\n",
            "Epoch 44, Loss: 1.557742517403882\n",
            "Epoch 45, Loss: 1.5407286641573665\n",
            "Epoch 46, Loss: 1.532540221406956\n",
            "Epoch 47, Loss: 1.5085103656306411\n",
            "Epoch 48, Loss: 1.4920236931906805\n",
            "Epoch 49, Loss: 1.4745958321022266\n",
            "Epoch 50, Loss: 1.4807340942247949\n",
            "Epoch 51, Loss: 1.4642729060818451\n",
            "Epoch 52, Loss: 1.4551649876315185\n",
            "Epoch 53, Loss: 1.444649885399173\n",
            "Epoch 54, Loss: 1.4283770190344915\n",
            "Epoch 55, Loss: 1.4242980456111407\n",
            "Epoch 56, Loss: 1.4105539948049217\n",
            "Epoch 57, Loss: 1.4051608930934558\n",
            "Epoch 58, Loss: 1.4004667563871904\n",
            "Epoch 59, Loss: 1.3864880017559937\n",
            "Epoch 60, Loss: 1.3722850276966287\n",
            "Epoch 61, Loss: 1.3743908212642477\n",
            "Epoch 62, Loss: 1.3644887437724105\n",
            "Epoch 63, Loss: 1.3609467913406064\n",
            "Epoch 64, Loss: 1.348193028960565\n",
            "Epoch 65, Loss: 1.3466622684941147\n",
            "Epoch 66, Loss: 1.3276901654522828\n",
            "Epoch 67, Loss: 1.3316033271828083\n",
            "Epoch 68, Loss: 1.3233677714762062\n",
            "Epoch 69, Loss: 1.3197179165753452\n",
            "Epoch 70, Loss: 1.2990674262094979\n",
            "Epoch 71, Loss: 1.3006449022678415\n",
            "Epoch 72, Loss: 1.2991753262702865\n",
            "Epoch 73, Loss: 1.2939590035062847\n",
            "Epoch 74, Loss: 1.2842169723125418\n",
            "Epoch 75, Loss: 1.2838311291704274\n",
            "Epoch 76, Loss: 1.2785884623575692\n",
            "Epoch 77, Loss: 1.270641070423704\n",
            "Epoch 78, Loss: 1.2663089518595223\n",
            "Epoch 79, Loss: 1.2594586562628698\n",
            "Epoch 80, Loss: 1.2654835395138673\n",
            "Epoch 81, Loss: 1.2539218676210655\n",
            "Epoch 82, Loss: 1.2440722747282549\n",
            "Epoch 83, Loss: 1.2404188459569758\n",
            "Epoch 84, Loss: 1.2260498747681126\n",
            "Epoch 85, Loss: 1.2316773274932244\n",
            "Epoch 86, Loss: 1.231522104956887\n",
            "Epoch 87, Loss: 1.2271685877231637\n",
            "Epoch 88, Loss: 1.2244373186670168\n",
            "Epoch 89, Loss: 1.211174326713639\n",
            "Epoch 90, Loss: 1.201202635813241\n",
            "Epoch 91, Loss: 1.2070369034102468\n",
            "Epoch 92, Loss: 1.1987235353450583\n",
            "Epoch 93, Loss: 1.1999638791036125\n",
            "Epoch 94, Loss: 1.201928947911118\n",
            "Epoch 95, Loss: 1.197690411047502\n",
            "Epoch 96, Loss: 1.1862448502068568\n",
            "Epoch 97, Loss: 1.187912936162467\n",
            "Epoch 98, Loss: 1.1854991310774678\n",
            "Epoch 99, Loss: 1.1813145574897226\n",
            "Epoch 100, Loss: 1.18161850505405\n",
            "she to her to as and to and she to deep on frustrated sudden on might the , encountered into , began shrink . frustrated a dream she to her as and their gathering engaging in riddles tales in riddles tales in home alice her confidence peculiar filled . alice\n",
            "Input: Alice was walking through the forest when she suddenly saw a strange.\n",
            "Generated: she , encountered variety exploration alice . she to through to stuck home alice her spirit one afternoon , chasing to through door she to entered beautiful but soon spiraling of , she to rapidly now enough fit the , took small at . her to through door alice their\n",
            "================================================================================\n",
            "Input: As Alice pleaded with the Queen for help\n",
            "Generated: of ruled into 's . the for short pleaded the , alice with unpredictable . for short and with knave second 's for she with , a commotion in of 's palace taller began march and with unpredictable . the queen help returning , spoke in of stood of knave\n",
            "================================================================================\n",
            "Input: Continuing her exploration, Alice encountered \n",
            "Generated: discovered wonderland in of . felt familiar of own . , by familiar of she in of . alice through door alice her and turmoil as pleaded the world wonderland as regained composure she her room confidence declared independence the queen as bolder , found in of own and she\n",
            "================================================================================\n",
            "Input: Alice delved into books\n",
            "Generated: after in returning , she hold key her she in rabbit and with queen grand , by familiar rabbit she that as and and their gathering she to mad of , by irrational , encountered an world in finally a for in she floating a she herself . her to\n",
            "================================================================================\n",
            "Input: First Citizen: First, you know Caius Marcius is chief enemy to the people.\n",
            "Generated: filled , encountered on fit the , caterpillar alice on to entered shrink . she a cheshire , encountered grinning cat who and the , time the , time the , encountered grinning cat but soon by talking hookah-smoking hearts hoping might the hatter tea , entered shrink . curiosity\n",
            "================================================================================\n",
            "Input: Alice began to explore lucid dreaming\n",
            "Generated: and her and she , entered , small to gathering in and her and her , began beautiful but discoveries when dare follow door world in riddles find queen a , to current . curiosity the hare the hatter tea . the world a , entered elusive , time through\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_sentence = \"How are you?\"\n",
        "# generated_text = generate_text(model, sentence, word_to_int, int_to_word, max_new_tokens=50)\n",
        "# generated_text"
      ],
      "metadata": {
        "id": "WUL8M3Ei_KEX"
      },
      "execution_count": 166,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eoY2CAyQPeWF",
        "pNfea0_8oLZw",
        "e8FHuZBKLfMP"
      ],
      "provenance": [],
      "mount_file_id": "1P40xsKBfsl3KKataojOyf-0VioRb8Syf",
      "authorship_tag": "ABX9TyOu10M6scD8+EXQxjBGNb7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}